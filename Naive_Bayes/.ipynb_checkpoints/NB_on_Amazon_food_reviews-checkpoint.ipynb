{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Naive Bayes to apply- Bernoulli, Multinomial or Gaussian?\n",
    "When we only care about a word being present or not in a document, then we use Bernoulli NB, if the frequency of words in a document is of interest and not just if it is present or not, then we use Multinomial NB. Gaussian NB is used when we have continuous real-valued features (values that can take any real values).\n",
    "\n",
    "For this problem, a given word can take values from a min 0 to a max of (num_of_documents_in_corpus), therefore we should use Multinomial NB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory to save pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pickle_files'):\n",
    "    os.mkdir('pickle_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test splitter for dataset sorted wrt time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X and y, before being passed to this function must be converted to numpy array or must be sparse matrices\\\n",
    "### for consistency throughout the program.\n",
    "\n",
    "### y will be a numpy vector because y-values originally are stored in a column of the original dataframe (ie, it\\\n",
    "### will be of type pd.Series. When converted into np-array, it will become a numpy 1D array, i.e a column vector)\n",
    "\n",
    "def train_test_splitter(X, y, test_size = 0.2):\n",
    "    train_size = 1 - test_size\n",
    "    \n",
    "    train_row_upper_index = round(train_size*X.shape[0])\n",
    "    test_row_lower_index = train_row_upper_index + 1\n",
    "    \n",
    "    if(X.ndim == 1):\n",
    "        X = X.reshape((X.shape[0], 1))\n",
    "    y = y.reshape((y.shape[0], 1)) # y is \n",
    "    \n",
    "    X_train = X[:train_row_upper_index + 1, :]\n",
    "    X_test = X[test_row_lower_index:, :]\n",
    "    \n",
    "    y_train = y[:train_row_upper_index + 1]\n",
    "    y_test = y[test_row_lower_index:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save best NB model\n",
    "This function can be reused. It can be added to any program to save intermediate best model trained during hyperparameter tuning (although it can save any python object in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model):\n",
    "\n",
    "    with open('pickle_files/nb_best.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(model, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f1_scores_and_y_pred_best(f1scores, y_pred):\n",
    "    \n",
    "    with open('pickle_files/f1scores.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(f1scores, pkl_file)\n",
    "        \n",
    "    with open('pickle_files/y_pred_best.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(y_pred, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1scores_vs_alpha(f1scores, alphas):\n",
    "    plt.scatter(alphas, f1scores, c = 'red')\n",
    "    plt.plot(alphas, f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "    \n",
    "    sns.heatmap(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune_alpha (laplace smoothing parameter, pseudocount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_alpha(alphas, X_train, y_train, X_cv, y_cv):\n",
    "    \n",
    "    best_alpha = None\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1scores =[]\n",
    "    \n",
    "    f1score_max = 0\n",
    "    best_alpha = None\n",
    "    y_pred_best = None\n",
    "    best_multinomial_nb = None\n",
    "    \n",
    "    if os.path.exists('pickle_files/nb_best.pkl'):\n",
    "        with open('pickle_files/nb_best.pkl', 'rb') as pkl_file:\n",
    "            multinomial_nb = pickle.load(pkl_file)\n",
    "            \n",
    "        ### if the model was already present, this means the f1scores and y_pred_best also must have been saved as well\n",
    "        with open('pickle_files/f1scores.pkl', 'rb') as pkl_file:\n",
    "            f1scores = pickle.load(pkl_file)\n",
    "        \n",
    "        with open('pickle_files/y_pred_best.pkl', 'rb') as pkl_file:\n",
    "            y_pred_best = pickle.load(pkl_file)\n",
    "        \n",
    "        plot_f1score_vs_alpha(f1scores, alphas)\n",
    "        plot_conf_matrix(y_cv, y_pred_best)\n",
    "        \n",
    "        return multinomial_nb\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        multinomial_nb = MultinomialNB(alpha = alpha, fit_prior = True, class_prior = None)\n",
    "        multinomial_nb = multinomial_nb.fit(X_train, y_train.ravel())\n",
    "        \n",
    "        y_pred = multinomial_nb.predict(X_cv)\n",
    "        \n",
    "        precisions.append(precision_score(y_cv, y_pred))\n",
    "        recalls.append(recall_score(y_cv, y_pred))\n",
    "        \n",
    "        f1score = f1_score(y_cv, y_pred)\n",
    "        f1scores.append(f1score)\n",
    "        \n",
    "        if(f1score > f1score_max):\n",
    "            f1score_max = f1score\n",
    "            y_pred_best = y_pred ### for building the confusion matrix\n",
    "            best_alpha = alpha\n",
    "            \n",
    "            ### this function will overwrite the previous best model\n",
    "            best_multinomial_nb = multinomial_nb\n",
    "            save_best_model(best_multinomial_nb)\n",
    "            \n",
    "    save_f1_scores_and_y_pred_best(f1scores, y_pred_best)\n",
    "    plot_f1score_vs_alpha(f1scores, alphas)\n",
    "    plot_conf_matrix(y_cv, y_pred_best)\n",
    "    \n",
    "    return best_multinomial_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('select * from reviews where Score != 3', db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing the ratings with 0 (for negative reviews) and 1 (for positive reviews).\n",
    " Score of  >3 has been considered as positive and a score of <3 has been taken as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    1\n",
       "2    4\n",
       "3    2\n",
       "4    5\n",
       "5    4\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(map(lambda x: 0 if x < 3 else 1, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Score'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Deduplication\n",
    "If a user id has multiple entries for the same timestamp, then it should be removed because it is likely that multiple entries at the same timestamp were for the same product of different variety which has a different product id than other variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197082"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(subset = ['UserId', 'Time']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = df.drop_duplicates(subset = ['UserId', 'Time'], inplace = False, keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328732, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Taking first 60k rows (only for the purpose of finishing this assignment assignment) after sorting wrt Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deduplicated_df.sort_values(by = 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:60000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extracting the data needed (corpus) and removing html and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset cleaners\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_html(sentence):\n",
    "    html_tag_re_obj = re.compile('<.*>?')\n",
    "    sentence = re.sub(html_tag_re_obj, ' ', sentence)\n",
    "#     amps_re = re.compile('&.+')\n",
    "#     sentence = re.sub(amps_re, ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z]', r' ', sentence)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "    cleaned_doc_1 = remove_html(doc)\n",
    "    cleaned_doc_2 = remove_punctuations(doc)\n",
    "    cleaned_corpus.append(cleaned_doc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21160\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Since the negative food reviews are likely to contain words like \"don't\", \"didn't\", etc that impart important\n",
    "## meaning to the review, we check if such words exist in the corpus that we have. If these words are in the corpus,\n",
    "## then they should not be in the list of stop words that we use for removing the stopwords from our corpus\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"not\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"don't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"didn't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the corpus to cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(map(lambda doc: doc.lower(), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this witty little book makes my son laugh at loud  i recite it in the car as we re driving along and he always can sing the refrain  he s learned about whales  india  drooping roses   i love all the new words this book  introduces and the silliness of it all   this is a classic book i am  willing to bet my son will still be able to recite from memory when he is  in college']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filtered_corpus = corpus with docs having no stop words\n",
    "### using lambda expression for this\n",
    "\n",
    "filtered_corpus = list(map(lambda doc: ' '.join(list(filter(lambda word: True if word not in stopwords else False\\\n",
    "                                                            , doc.split()))), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['witty little book makes son laugh loud recite car driving along always sing refrain learned whales india drooping roses love new words book introduces silliness classic book willing bet son still able recite memory college',\n",
       " 'remember seeing show aired television years ago child sister later bought lp day thirty something used series books amp songs student teaching preschoolers amp turned whole school purchasing cd along books children amp tradition lives']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming the words (SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus = list(map(lambda doc: ' '.join(list(map(stemmer.stem, doc.split()))), filtered_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['witti littl book make son laugh loud recit car drive along alway sing refrain learn whale india droop rose love new word book introduc silli classic book will bet son still abl recit memori colleg',\n",
       " 'rememb see show air televis year ago child sister later bought lp day thirti someth use seri book amp song student teach preschool amp turn whole school purchas cd along book children amp tradit live',\n",
       " 'beetlejuic well written movi everyth excel act special effect delight chose view movi']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_filtered_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = stemmed_filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train, cv and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train_test splitter takes only numpy arrays and sparse matrices as arguments\n",
    "corpus = np.array(corpus)\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nought, y_train_nought, X_test, y_test = train_test_splitter(corpus, scores, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv = train_test_splitter(X_train_nought, y_train_nought, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: <class 'numpy.ndarray'> y_train: <class 'numpy.ndarray'> X_cv: <class 'numpy.ndarray'> y_cv: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(type(X_train)), 'y_train: ' + str(type(y_train)), 'X_cv: '+str(type(X_cv)), \\\n",
    "      'y_cv: ' + str(type(y_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38402, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words (CountVectorizer)\n",
    "##### Note: Vectorization must be done on training set only, not on test set or the cross validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pickle_files/bow_pickles'):\n",
    "    os.mkdir('pickle_files/bow_pickles')\n",
    "    \n",
    "if os.path.exists('pickle_files/bow_pickles/document_term_matrix.pkl'):\n",
    "    with open('pickle_files/bow_pickles/document_term_matrix.pkl', 'rb') as dtm_pickle:\n",
    "        X_train_bow = pickle.load(dtm_pickle)\n",
    "    with open('pickle_files/bow_pickles/count_vectorizer.pkl', 'rb') as vectorizer:\n",
    "        count_vectorizer = pickle.load(vectorizer)\n",
    "        \n",
    "else:\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "#     fit() method takes 1D array (m,). train_test_splitter() returns (m,n) array. ravel() converts it into (m,)\n",
    "    count_vectorizer = count_vectorizer.fit(X_train.ravel())\n",
    "    X_train_bow = count_vectorizer.transform(X_train.ravel()) # document_term_matrix is saved as X_train_bow\n",
    "    with open('pickle_files/bow_pickles/document_term_matrix.pkl', 'wb') as dtm_pickle:\n",
    "        pickle.dump(X_train_bow, dtm_pickle)\n",
    "    with open('pickle_files/bow_pickles/count_vectorizer.pkl', 'wb') as vectorizer:\n",
    "        pickle.dump(count_vectorizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38402, 25356)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TfIdf (TfIdfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pickle_files/tfidf_pickles'):\n",
    "    os.mkdir('pickle_files/tfidf_pickles')\n",
    "    \n",
    "if os.path.exists('pickle_files/tfidf_pickles/document_term_matrix.pkl'):\n",
    "    with open('pickle_files/tfidf_pickles/document_term_matrix.pkl', 'rb') as dtm_pickle:\n",
    "        X_train_tfidf = pickle.load(dtm_pickle)\n",
    "    with open('pickle_files/tfidf_pickles/tfidf_vectorizer.pkl', 'rb') as vectorizer:\n",
    "        tfidf_vectorizer = pickle.load(vectorizer)\n",
    "        \n",
    "else:\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer = tfidf_vectorizer.fit(X_train.ravel())\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train.ravel())\n",
    "    with open('pickle_files/tfidf_pickles/document_term_matrix.pkl', 'wb') as dtm_pickle:\n",
    "        pickle.dump(X_train_tfidf, dtm_pickle)\n",
    "    with open('pickle_files/tfidf_pickles/tfidf_vectorizer.pkl', 'wb') as vectorizer:\n",
    "        pickle.dump(tfidf_vectorizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38402, 25356)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes for BoW (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_bow = count_vectorizer.transform(X_cv.ravel())\n",
    "X_test_bow = count_vectorizer.transform(X_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38402, 1) (9599, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickle_files/nb_best.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d48e5184c300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_cv_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-3df30378569c>\u001b[0m in \u001b[0;36mtune_alpha\u001b[0;34m(alphas, X_train, y_train, X_cv, y_cv)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m### this function will overwrite the previous best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mbest_multinomial_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultinomial_nb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0msave_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_multinomial_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0msave_f1_scores_and_y_pred_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8c41eeb5179>\u001b[0m in \u001b[0;36msave_best_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pickle_files/nb_best.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickle_files/nb_best.pkl'"
     ]
    }
   ],
   "source": [
    "alpha = tune_alpha(alphas = [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100], \\\n",
    "                   X_train = X_train_bow, y_train = y_train, X_cv = X_cv_bow, y_cv = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['database.sqlite', '.ipynb_checkpoints', 'NB_on_Amazon_food_reviews.ipynb']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
