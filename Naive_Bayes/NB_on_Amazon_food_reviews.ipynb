{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Naive Bayes to apply- Bernoulli, Multinomial or Gaussian?\n",
    "When we only care about a word being present or not in a document, then we use Bernoulli NB, if the frequency of words in a document is of interest and not just if it is present or not, then we use Multinomial NB. Gaussian NB is used when we have continuous real-valued features (values that can take any real values).\n",
    "\n",
    "For this problem, a given word can take values from a min 0 to a max of (num_of_documents_in_corpus), therefore we should use Multinomial NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test splitter for dataset sorted wrt time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X and y, before being passed to this function must be converted to numpy array or must be sparse matrices\\\n",
    "### for consistency throughout the program.\n",
    "\n",
    "### y will be a numpy vector because y-values originally are stored in a column of the original dataframe (ie, it\\\n",
    "### will be of type pd.Series. When converted into np-array, it will become a numpy 1D array, i.e a column vector)\n",
    "\n",
    "def train_test_splitter(X, y, test_size = 0.2):\n",
    "    train_size = 1 - test_size\n",
    "    \n",
    "    train_row_upper_index = round(train_size*X.shape[0])\n",
    "    test_row_lower_index = train_row_upper_index + 1\n",
    "    \n",
    "    if(X.ndim == 1):\n",
    "        X = X.reshape((X.shape[0], 1))\n",
    "    y = y.reshape((y.shape[0], 1)) # y is \n",
    "    \n",
    "    X_train = X[:train_row_upper_index + 1, :]\n",
    "    X_test = X[test_row_lower_index:, :]\n",
    "    \n",
    "    y_train = y[:train_row_upper_index + 1]\n",
    "    y_test = y[test_row_lower_index:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save best NB model\n",
    "This function can be reused. It can be added to any program to save intermediate best model trained during hyperparameter tuning (although it can save any python object in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model):\n",
    "\n",
    "    with open('pickle_files/nb_best.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(model, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f1_scores_and_y_pred_best(f1scores, y_pred):\n",
    "    \n",
    "    with open('pickle_files/f1scores.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(f1scores, pkl_file)\n",
    "        \n",
    "    with open('pickle_files/y_pred_best.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(y_pred, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1scores_vs_alpha(f1scores, alphas):\n",
    "    \n",
    "    plt.plot(alphas, f1scores)\n",
    "    \n",
    "    max_f1score = max(f1scores)\n",
    "    alpha_corr_to_max_f1score = alphas[f1scores.index(max_f1score)]\n",
    "    \n",
    "    f1scores.remove(max_f1score)\n",
    "    alphas.remove(alpha_corr_to_max_f1score)\n",
    "    plt.title('F1-scores vs alpha')\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('f1scores')\n",
    "    plt.scatter(alphas, f1scores, c = 'red')\n",
    "    \n",
    "    plt.scatter(alpha_corr_to_max_f1score, max_f1score, c = 'yellow', s = 50, edgecolors = 'black', marker = 'D',\\\n",
    "               label = '(' + str(alpha_corr_to_max_f1score) + ', ' + str(max_f1score) + ')')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "    \n",
    "    sns.heatmap(conf_matrix_df, annot = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune_alpha (laplace smoothing parameter, pseudocount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_alpha(alphas, X_train, y_train, X_cv, y_cv):\n",
    "    \n",
    "    best_alpha = None\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1scores =[]\n",
    "    \n",
    "    f1score_max = 0\n",
    "    best_alpha = None\n",
    "    y_pred_best = None\n",
    "    best_multinomial_nb = None\n",
    "    \n",
    "    if not os.path.exists('pickle_files'):\n",
    "        os.mkdir('pickle_files')\n",
    "    \n",
    "    if os.path.exists('pickle_files/nb_best.pkl'):\n",
    "        with open('pickle_files/nb_best.pkl', 'rb') as pkl_file:\n",
    "            multinomial_nb = pickle.load(pkl_file)\n",
    "            \n",
    "        ### if the model was already present, this means the f1scores and y_pred_best also must have been saved as well\n",
    "        with open('pickle_files/f1scores.pkl', 'rb') as pkl_file:\n",
    "            f1scores = pickle.load(pkl_file)\n",
    "        \n",
    "        with open('pickle_files/y_pred_best.pkl', 'rb') as pkl_file:\n",
    "            y_pred_best = pickle.load(pkl_file)\n",
    "        \n",
    "        plot_f1scores_vs_alpha(f1scores, alphas)\n",
    "        plot_conf_matrix(y_cv, y_pred_best)\n",
    "        \n",
    "        return multinomial_nb\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        multinomial_nb = MultinomialNB(alpha = alpha, fit_prior = True, class_prior = None)\n",
    "        multinomial_nb = multinomial_nb.fit(X_train, y_train.ravel())\n",
    "        \n",
    "        y_pred = multinomial_nb.predict(X_cv)\n",
    "        \n",
    "        precisions.append(precision_score(y_cv, y_pred))\n",
    "        recalls.append(recall_score(y_cv, y_pred))\n",
    "        \n",
    "        f1score = f1_score(y_cv, y_pred)\n",
    "        f1scores.append(f1score)\n",
    "        \n",
    "        if(f1score > f1score_max):\n",
    "            f1score_max = f1score\n",
    "            y_pred_best = y_pred ### for building the confusion matrix\n",
    "            best_alpha = alpha\n",
    "            \n",
    "            ### this function will overwrite the previous best model\n",
    "            best_multinomial_nb = multinomial_nb\n",
    "            save_best_model(best_multinomial_nb)\n",
    "            \n",
    "    save_f1_scores_and_y_pred_best(f1scores, y_pred_best)\n",
    "    plot_f1scores_vs_alpha(f1scores, alphas)\n",
    "    plot_conf_matrix(y_cv, y_pred_best)\n",
    "    \n",
    "    return best_multinomial_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('select * from reviews where Score != 3', db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing the ratings with 0 (for negative reviews) and 1 (for positive reviews).\n",
    " Score of  >3 has been considered as positive and a score of <3 has been taken as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(map(lambda x: 0 if x < 3 else 1, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Score'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Deduplication\n",
    "If a user id has multiple entries for the same timestamp, then it should be removed because it is likely that multiple entries at the same timestamp were for the same product of different variety which has a different product id than other variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset = ['UserId', 'Time']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = df.drop_duplicates(subset = ['UserId', 'Time'], inplace = False, keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Taking first 60k rows (only for the purpose of finishing this assignment assignment) after sorting wrt Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deduplicated_df.sort_values(by = 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:100000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extracting the data needed (corpus) and removing html and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset cleaners\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_html(sentence):\n",
    "    html_tag_re_obj = re.compile('<.*>?')\n",
    "    sentence = re.sub(html_tag_re_obj, ' ', sentence)\n",
    "#     amps_re = re.compile('&.+')\n",
    "#     sentence = re.sub(amps_re, ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z]', r' ', sentence)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "    cleaned_doc_1 = remove_html(doc)\n",
    "    cleaned_doc_2 = remove_punctuations(doc)\n",
    "    cleaned_corpus.append(cleaned_doc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the negative food reviews are likely to contain words like \"don't\", \"didn't\", etc that impart important\n",
    "## meaning to the review, we check if such words exist in the corpus that we have. If these words are in the corpus,\n",
    "## then they should not be in the list of stop words that we use for removing the stopwords from our corpus\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"not\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"don't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"didn't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the corpus to cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'not' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(map(lambda doc: doc.lower(), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filtered_corpus = corpus with docs having no stop words\n",
    "### using lambda expression for this\n",
    "\n",
    "filtered_corpus = list(map(lambda doc: ' '.join(list(filter(lambda word: True if word not in stopwords else False\\\n",
    "                                                            , doc.split()))), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming the words (SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus = list(map(lambda doc: ' '.join(list(map(stemmer.stem, doc.split()))), filtered_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = stemmed_filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train, cv and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train_test splitter takes only numpy arrays and sparse matrices as arguments\n",
    "corpus = np.array(corpus)\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nought, y_train_nought, X_test, y_test = train_test_splitter(corpus, scores, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv = train_test_splitter(X_train_nought, y_train_nought, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train: ' + str(type(X_train)), 'y_train: ' + str(type(y_train)), 'X_cv: '+str(type(X_cv)), \\\n",
    "      'y_cv: ' + str(type(y_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words (CountVectorizer)\n",
    "##### Note: Vectorization must be done on training set only, not on test set or the cross validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pickle_files/bow_pickles'):\n",
    "    os.mkdir('pickle_files/bow_pickles')\n",
    "    \n",
    "if os.path.exists('pickle_files/bow_pickles/document_term_matrix.pkl'):\n",
    "    with open('pickle_files/bow_pickles/document_term_matrix.pkl', 'rb') as dtm_pickle:\n",
    "        X_train_bow = pickle.load(dtm_pickle)\n",
    "    with open('pickle_files/bow_pickles/count_vectorizer.pkl', 'rb') as vectorizer:\n",
    "        count_vectorizer = pickle.load(vectorizer)\n",
    "        \n",
    "else:\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "#     fit() method takes 1D array (m,). train_test_splitter() returns (m,n) array. ravel() converts it into (m,)\n",
    "    count_vectorizer = count_vectorizer.fit(X_train.ravel())\n",
    "    X_train_bow = count_vectorizer.transform(X_train.ravel()) # document_term_matrix is saved as X_train_bow\n",
    "    with open('pickle_files/bow_pickles/document_term_matrix.pkl', 'wb') as dtm_pickle:\n",
    "        pickle.dump(X_train_bow, dtm_pickle)\n",
    "    with open('pickle_files/bow_pickles/count_vectorizer.pkl', 'wb') as vectorizer:\n",
    "        pickle.dump(count_vectorizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TfIdf (TfIdfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pickle_files/tfidf_pickles'):\n",
    "    os.mkdir('pickle_files/tfidf_pickles')\n",
    "    \n",
    "if os.path.exists('pickle_files/tfidf_pickles/document_term_matrix.pkl'):\n",
    "    with open('pickle_files/tfidf_pickles/document_term_matrix.pkl', 'rb') as dtm_pickle:\n",
    "        X_train_tfidf = pickle.load(dtm_pickle)\n",
    "    with open('pickle_files/tfidf_pickles/tfidf_vectorizer.pkl', 'rb') as vectorizer:\n",
    "        tfidf_vectorizer = pickle.load(vectorizer)\n",
    "        \n",
    "else:\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer = tfidf_vectorizer.fit(X_train.ravel())\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train.ravel())\n",
    "    with open('pickle_files/tfidf_pickles/document_term_matrix.pkl', 'wb') as dtm_pickle:\n",
    "        pickle.dump(X_train_tfidf, dtm_pickle)\n",
    "    with open('pickle_files/tfidf_pickles/tfidf_vectorizer.pkl', 'wb') as vectorizer:\n",
    "        pickle.dump(tfidf_vectorizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes for BoW (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_bow = count_vectorizer.transform(X_cv.ravel())\n",
    "X_test_bow = count_vectorizer.transform(X_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = tune_alpha(alphas = [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100], \\\n",
    "                   X_train = X_train_bow, y_train = y_train, X_cv = X_cv_bow, y_cv = y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding TRP, FPR, TNR, FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading the y_pred predicted by the best alpha value (saved as pickle by tune_alpha() method)\n",
    "with open('pickle_files/y_pred_best.pkl', 'rb') as pkl_file:\n",
    "    y_pred_best = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_cv, y_pred_best).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp/(tp+fn) # true_positive_predicted per acutal_positive\n",
    "fpr = fp/(tn+fp) # false_positive_predicted per actual_negative\n",
    "fnr = fn/(tp+fn) # false_negative_prediceted per actual_positive\n",
    "tnr = tn/(tn+fp) # true_negative_predicted per actual_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print('tpr: ' + str(tpr))\n",
    "print('fpr: ' + str(fpr))\n",
    "print('fnr: ' + str(fnr))\n",
    "print('tnr: ' + str(tnr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
