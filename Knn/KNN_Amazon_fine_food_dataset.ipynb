{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tutorial_ipynbs',\n",
       " '.gitignore.swp',\n",
       " '.gitignore',\n",
       " 'database.sqlite',\n",
       " '.ipynb_checkpoints',\n",
       " 'KNN_Amazon_fine_food_dataset.ipynb']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisable_dataset = pd.read_sql_query('select * from reviews where Score != 3', db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisable_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1303862400\n",
       "1    1346976000\n",
       "2    1219017600\n",
       "3    1307923200\n",
       "4    1350777600\n",
       "Name: Time, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisable_dataset['Time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisable_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = polarisable_dataset # just to make typing easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df.sample(n = 5000, axis = 0, random_state = 0) # random sampling of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing the ratings with 0 (for negative reviews) and 1 (for positive reviews).\n",
    "#### Score of >3 has been considered as positive and a score of <3 has been taken as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampled_df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sampled_df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161937    5\n",
       "451123    5\n",
       "401436    5\n",
       "396416    5\n",
       "449554    1\n",
       "379099    1\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(map(lambda x: 0 if x<3 else 1, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['Score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampled_df['Score'].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Deduplication\n",
    "If a user id has multiple entries for the same timestamp, then it should be removed because it is likely that multiple entries at the same timestamp were for the same product of different variety which has a different product id than other variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.duplicated(subset = ['UserId', 'Time']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_deduplicated_df = sampled_df.drop_duplicates(subset = ['UserId', 'Time'], inplace = False, keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting the data needed (corpus)\n",
    "#### And removing html and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sampled_deduplicated_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset cleaners\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_html(sentence):\n",
    "    html_tag_re_obj = re.compile('<.*>?')\n",
    "    return re.sub(html_tag_re_obj, ' ', sentence)\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z]', r' ', sentence)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "    cleaned_doc_1 = remove_html(doc)\n",
    "    cleaned_doc_2 = remove_punctuations(doc)\n",
    "    cleaned_corpus.append(cleaned_doc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Since the negative food reviews are likely to contain words like \"don't\", \"didn't\", etc that impart important\n",
    "## meaning to the review, we check if such words exist in the corpus that we have. If these words are in the corpus,\n",
    "## then they should not be in the list of stop words that we use for removing the stopwords from our corpus\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"not\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"don't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"didn't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,0,1,0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_corpus = corpus with docs having no stop words\n",
    "# doing with the sexy lambda expression\n",
    "\n",
    "filtered_corpus = list(map(lambda doc: ' '.join(list(filter(lambda word: True if word not in stopwords else False\\\n",
    "                                                            , doc.split()))), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4938"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We pretty much given GF pasta restaurant. It's really good disintegrate toss sauce. My celiac husband thrilled! FYI, first ingredient CORN, quinoa second, not QUITE nutritious might think -- cares!\",\n",
       " 'I bought first SproutMaster years ago Dowling Orchard Market passing Banning, California (back sold them).<br /><br />Through bit pricey plastic tray, I cannot complain ease use quality sprouts grown. The divider IS helpful grow different sprouts, BUT helpful grow type sprout two different stages growth. This way, always fresh sprouts handy anytime.<br /><br />They grow fast Sprout Master makes simple care them.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### classical way of removing the lambda expressions\n",
    "### verified the output of lambda expression output with the output of following implementation, outputs are same\n",
    "# docs_without_stop_words = []\n",
    "# for i, doc in enumerate(corpus):\n",
    "#     non_stop_words_in_doc = []\n",
    "#     for word in doc.split():\n",
    "#         if word not in stopwords:\n",
    "#             non_stop_words_in_doc.append(word)\n",
    "            \n",
    "    \n",
    "#     docs_without_stop_words.append(' '.join(non_stop_words_in_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming the words (SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus = list(map(lambda doc: ' '.join(list(map(stemmer.stem, doc.split()))), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"we had pretti much given up on gf pasta until we had this in a restaurant. it realli good and doesn't disintegr when you toss it with sauce. my celiac husband is thrilled! fyi, the first ingredi is corn, and quinoa is second, so it not quit as nutriti as you might think -- but who cares!\",\n",
       " 'i bought my first sproutmast year ago at the dowl orchard market when pass through banning, california (back when they sold them).<br /><br />through a bit pricey for a plastic tray, i cannot complain about the eas of use or the qualiti of the sprout grown. the divid is help to grow differ sprouts, but it is more help to grow the same type of sprout in two differ stage of growth. this way, you can alway have fresh sprout handi at anytime.<br /><br />they do grow fast and the sprout master make it so simpl to care for them.',\n",
       " 'for bold coffe fan like us, this is absolut delici coffee. it has a complex taste, a hint of someth resembl red wine. i almost hesit to review it becaus it will probabl caus the price to go up or avail to go down.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_filtered_corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the dataset according to Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_deduplicated_df['Text'] = stemmed_filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = sampled_deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_sorted = working_df.sort_values(by = 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus_sorted = working_df_sorted['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the reviews and splitting into train, cv and test sets and TRAINING and TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Bag of Words (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix = count_vectorizer.fit_transform(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4938, 13452)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = working_df_sorted['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Splitting into train, cv and test (Simple Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will not work because train_test_split() splits data randomly. What we want is a time-based splitting on\n",
    "### the dataset that we have sorted chronologically\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4938, 13452) (4938,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * ## Function to split dataset into train and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(X, y, test_size):\n",
    "    train_size = 1 - test_size\n",
    "    train_row_upper_index = round(train_size*X.shape[0])\n",
    "    test_row_lower_index = train_row_upper_index + 1\n",
    "    \n",
    "#     print(train_row_upper_index)\n",
    "    \n",
    "    X_train = X[:train_row_upper_index + 1, :]\n",
    "    X_test = X[test_row_lower_index:, :]\n",
    "    y_train = y.iloc[:train_row_upper_index + 1]\n",
    "    y_test = y.iloc[test_row_lower_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_splitter(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4938"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the previous X_train and y_train into X_train, X_cv and y_train, y_cv\n",
    "X_train, X_cv, y_train, y_cv = train_test_splitter(X_train, y_train, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## KNN Classifier and cross validator using Simple Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_trainer_and_cross_validator(k):\n",
    "    \n",
    "    # Side note: note that X_train and y_train are sparse matrices, and not numpy arrays\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_cv = knn.predict(X_cv)\n",
    "    \n",
    "    accuracy = accuracy_score(y_cv, y_pred_cv) * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 61 is: 83.13513513513513\n",
      "Accuracy for k = 63 is: 83.13513513513513\n",
      "Accuracy for k = 65 is: 83.13513513513513\n",
      "Accuracy for k = 67 is: 83.13513513513513\n",
      "Accuracy for k = 69 is: 83.13513513513513\n",
      "Accuracy for k = 71 is: 83.13513513513513\n",
      "Accuracy for k = 73 is: 83.13513513513513\n",
      "Accuracy for k = 75 is: 83.13513513513513\n",
      "Accuracy for k = 77 is: 83.13513513513513\n",
      "Accuracy for k = 79 is: 83.13513513513513\n",
      "Accuracy for k = 81 is: 83.13513513513513\n",
      "Accuracy for k = 83 is: 83.13513513513513\n",
      "Accuracy for k = 85 is: 83.13513513513513\n",
      "Accuracy for k = 87 is: 83.13513513513513\n",
      "Accuracy for k = 89 is: 83.13513513513513\n"
     ]
    }
   ],
   "source": [
    "## tried for k = 1 till 30 and found that for odd k values in this range, accuracy always came below 86%.\n",
    "## accuracy for k = 31 to 41 (inclusive) was same, 86.8876% and from 43 to 59 it was 86.7435%.\n",
    "## accuracy for k = 61 to 90 is clearly less than the accuracy obtained for k = 31 to 41.\n",
    "## we shall choose one of the k values from k = 31 to 41\n",
    "\n",
    "for i in range(61, 90, 2):\n",
    "    accuracy = knn_trainer_and_cross_validator(i)\n",
    "    print('Accuracy for k = ' + str(i) + ' is: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Since the cross-validation accuracy obtained for k = 31, 33, 35, 37, 39 and 41 are same and max, we choose\n",
    "##### k = 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=33, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### finally testing after tuning the hyperparameter 'k' on the cross validation set\n",
    "y_pred_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.29278183292782"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = tfidf_vectorizer.fit(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix = tfidf_vectorizer.transform(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4938, 13452)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = working_df_sorted['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_first, X_test, y_train_first, y_test = train_test_splitter(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_splitter(X_train_first, y_train_first, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1 is 77.1891891891892\n",
      "Accuracy for k = 3 is 82.16216216216216\n",
      "Accuracy for k = 5 is 82.91891891891892\n",
      "Accuracy for k = 7 is 83.56756756756756\n",
      "Accuracy for k = 9 is 83.24324324324324\n",
      "Accuracy for k = 11 is 83.24324324324324\n",
      "Accuracy for k = 13 is 83.56756756756756\n",
      "Accuracy for k = 15 is 83.35135135135135\n",
      "Accuracy for k = 17 is 83.35135135135135\n",
      "Accuracy for k = 19 is 83.35135135135135\n",
      "Accuracy for k = 21 is 83.24324324324324\n",
      "Accuracy for k = 23 is 83.24324324324324\n",
      "Accuracy for k = 25 is 83.24324324324324\n",
      "Accuracy for k = 27 is 83.24324324324324\n",
      "Accuracy for k = 29 is 83.24324324324324\n",
      "Accuracy for k = 31 is 83.24324324324324\n",
      "Accuracy for k = 33 is 83.35135135135135\n",
      "Accuracy for k = 35 is 83.24324324324324\n",
      "Accuracy for k = 37 is 83.24324324324324\n",
      "Accuracy for k = 39 is 83.24324324324324\n",
      "Accuracy for k = 41 is 83.24324324324324\n",
      "Accuracy for k = 43 is 83.24324324324324\n",
      "Accuracy for k = 45 is 83.13513513513513\n",
      "Accuracy for k = 47 is 83.13513513513513\n",
      "Accuracy for k = 49 is 83.13513513513513\n",
      "Accuracy for k = 51 is 83.13513513513513\n",
      "Accuracy for k = 53 is 83.13513513513513\n",
      "Accuracy for k = 55 is 83.13513513513513\n",
      "Accuracy for k = 57 is 83.13513513513513\n",
      "Accuracy for k = 59 is 83.13513513513513\n",
      "Accuracy for k = 61 is 83.13513513513513\n",
      "Accuracy for k = 63 is 83.13513513513513\n",
      "Accuracy for k = 65 is 83.13513513513513\n",
      "Accuracy for k = 67 is 83.13513513513513\n",
      "Accuracy for k = 69 is 83.13513513513513\n",
      "Accuracy for k = 71 is 83.13513513513513\n",
      "Accuracy for k = 73 is 83.13513513513513\n",
      "Accuracy for k = 75 is 83.13513513513513\n",
      "Accuracy for k = 77 is 83.13513513513513\n",
      "Accuracy for k = 79 is 83.13513513513513\n",
      "Accuracy for k = 81 is 83.13513513513513\n",
      "Accuracy for k = 83 is 83.13513513513513\n",
      "Accuracy for k = 85 is 83.13513513513513\n",
      "Accuracy for k = 87 is 83.13513513513513\n",
      "Accuracy for k = 89 is 83.13513513513513\n",
      "Accuracy for k = 91 is 83.13513513513513\n",
      "Accuracy for k = 93 is 83.13513513513513\n",
      "Accuracy for k = 95 is 83.13513513513513\n",
      "Accuracy for k = 97 is 83.13513513513513\n",
      "Accuracy for k = 99 is 83.13513513513513\n"
     ]
    }
   ],
   "source": [
    "accuracies_for_diff_k = []\n",
    "for i in range(1, 100, 2):\n",
    "    accuracy = knn_trainer_and_cross_validator(i)\n",
    "    accuracies_for_diff_k.append(accuracy)\n",
    "    print('Accuracy for k = ' + str(i) + ' is ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.56756756756756\t3\n"
     ]
    }
   ],
   "source": [
    "print(max(accuracies_for_diff_k), accuracies_for_diff_k.index(max(accuracies_for_diff_k)), sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### k = 3 had the best accuracy on cross validation set, so our hyperparameter k is tuned to be taken as 3\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the final 3-NN\n",
    "\n",
    "knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing on test data\n",
    "\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.58961881589619"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### accuracy score\n",
    "\n",
    "accuracy_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Average W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tokenizing each document in the corpus\n",
    "#### gensim w2v requires each document to be tokenized into words. The corpus will be a list of lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemmed_filtered_corpus_sorted is a pandas Series object. It should be converted into a list first.\n",
    "## after that each sentence in the resulted list should be tokenized into words stored in a list.\n",
    "## all these lists should be stored into another list so as to give a list of lists as required by gensim w2v\n",
    "\n",
    "stemmed_filtered_corpus_sorted_list = list(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_sorted_list_of_tokenized_sentences = []\n",
    "\n",
    "for sentence in stemmed_filtered_corpus_sorted:\n",
    "    tokenized_sentence = sentence.split()\n",
    "    stemmed_filtered_sorted_list_of_tokenized_sentences.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(stemmed_filtered_sorted_list_of_tokenized_sentences, min_count = 1, size = 50, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23448707,  0.3602921 , -0.04205541,  0.24961829,  0.06703568,\n",
       "       -0.56954414, -0.04139027, -0.10449755, -0.24704807,  0.3226019 ,\n",
       "       -0.16563085, -0.06909975,  0.12574764,  0.7040844 ,  0.03780265,\n",
       "        0.80083275, -0.12604529, -0.5191217 ,  0.08033808, -0.0251145 ,\n",
       "        0.39306724,  0.41053078,  0.04081402,  0.20275305, -0.55352265,\n",
       "       -0.14907712,  0.25555494, -0.18186875, -0.30055696, -0.39865583,\n",
       "       -0.58898985,  0.06406677, -0.00765801, -0.01547717, -0.13172501,\n",
       "        0.2741572 ,  0.08689801, -0.06458356, -0.21395421,  0.06347103,\n",
       "        0.31692132, -0.01164706, -0.21336176,  0.19902211, -0.01482673,\n",
       "       -0.14805146, -0.04206163,  0.53220314, -0.2595097 , -0.19508953],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['happen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the w2v model for later use\n",
    "\n",
    "import os\n",
    "\n",
    "if(not os.path.exists('w2v_practice.model')):\n",
    "    w2v.save('w2v_practice.model')\n",
    "    \n",
    "else:\n",
    "    w2v = gensim.models.Word2Vec.load('w2v_practice.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
