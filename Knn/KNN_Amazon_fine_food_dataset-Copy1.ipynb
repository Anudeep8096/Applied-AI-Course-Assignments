{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * ## Function to split dataset into train and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X may be sparse matrix or pd.DataFrame\n",
    "### y is pd.Series\n",
    "def train_test_splitter(X, y, test_size, return_only_training_split = False):\n",
    "    train_size = 1 - test_size\n",
    "    train_row_upper_index = round(train_size*X.shape[0])\n",
    "    test_row_lower_index = train_row_upper_index + 1\n",
    "    \n",
    "    print('X     y\\t', X.shape, y.shape)\n",
    "#     print(train_row_upper_index)\n",
    "#     print(test_row_lower_index)\n",
    "#     print(type(train_row_upper_index))\n",
    "#     print(type(test_row_lower_index))\n",
    "#     print(type(X))\n",
    "\n",
    "    X_train = X[:train_row_upper_index + 1]\n",
    "    X_test = X[test_row_lower_index:]\n",
    "    \n",
    "#     if y is not None:\n",
    "#         y_train = y.iloc[:train_row_upper_index + 1]\n",
    "#         y_test = y.iloc[test_row_lower_index:]\n",
    "    \n",
    "    y_train = y.iloc[:train_row_upper_index + 1]\n",
    "    y_test = y.iloc[test_row_lower_index:]\n",
    "    \n",
    "    if return_only_training_split == True:\n",
    "        return X_train, y_train\n",
    "    \n",
    "    print(X_test.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## KNN Classifier and cross validator using Simple Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_trainer_and_cross_validator(k, X_train, y_train, X_cv, y_cv, algorithm, save_name):\n",
    "    \n",
    "    # Side note: note that X_train and y_train are sparse matrices, and not numpy arrays\n",
    "#     knn = KNeighborsClassifier(n_neighbors = k, algorithm = algorithm)\n",
    "#     knn.fit(X_train, y_train)\n",
    "    \n",
    "    if os.path.exists(save_name + '.pkl'):\n",
    "        with open(save_name + '.pkl', 'rb') as trained_knn_pkl:\n",
    "            knn = pickle.load(trained_knn_pkl)\n",
    "    else:\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, algorithm = algorithm)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        with open(save_name + '.pkl', 'wb') as trained_knn_pkl:\n",
    "            pickle.dump(knn, trained_knn_pkl)\n",
    "        \n",
    "    \n",
    "    y_pred_cv = knn.predict(X_cv)\n",
    "    \n",
    "    f1score = f1_score(y_cv, y_pred_cv) * 100\n",
    "    \n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisable_dataset = pd.read_sql_query('select * from reviews where Score != 3', db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisable_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1303862400\n",
       "1    1346976000\n",
       "2    1219017600\n",
       "3    1307923200\n",
       "4    1350777600\n",
       "Name: Time, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisable_dataset['Time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisable_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = polarisable_dataset # just to make typing easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sampled_df contains top 100k examples from the dataset (instead of doing random sampling)\n",
    "\n",
    "sampled_df = df.iloc[:10000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing the ratings with 0 (for negative reviews) and 1 (for positive reviews).\n",
    "#### Score of >3 has been considered as positive and a score of <3 has been taken as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampled_df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sampled_df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     5\n",
       "7     5\n",
       "8     5\n",
       "9     5\n",
       "10    5\n",
       "11    5\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(map(lambda x: 0 if x<3 else 1, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['Score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampled_df['Score'].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Deduplication\n",
    "If a user id has multiple entries for the same timestamp, then it should be removed because it is likely that multiple entries at the same timestamp were for the same product of different variety which has a different product id than other variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.duplicated(subset = ['UserId', 'Time']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_deduplicated_df = sampled_df.drop_duplicates(subset = ['UserId', 'Time'], inplace = False, keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting the data needed (corpus)\n",
    "#### And removing html and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sampled_deduplicated_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset cleaners\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_html(sentence):\n",
    "    html_tag_re_obj = re.compile('<.*>?')\n",
    "    return re.sub(html_tag_re_obj, ' ', sentence)\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z]', r' ', sentence)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "    cleaned_doc_1 = remove_html(doc)\n",
    "    cleaned_doc_2 = remove_punctuations(doc)\n",
    "    cleaned_corpus.append(cleaned_doc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3544\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Since the negative food reviews are likely to contain words like \"don't\", \"didn't\", etc that impart important\n",
    "## meaning to the review, we check if such words exist in the corpus that we have. If these words are in the corpus,\n",
    "## then they should not be in the list of stop words that we use for removing the stopwords from our corpus\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"not\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"don't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in cleaned_corpus:\n",
    "    if \"didn't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,0,1,0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_corpus = corpus with docs having no stop words\n",
    "# doing with the sexy lambda expression\n",
    "\n",
    "filtered_corpus = list(map(lambda doc: ' '.join(list(filter(lambda word: True if word not in stopwords else False\\\n",
    "                                                            , doc.split()))), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9473"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I bought several Vitality canned dog food products found good quality. The product looks like stew processed meat smells better. My Labrador finicky appreciates product better most.',\n",
       " 'Product arrived labeled Jumbo Salted Peanuts...the peanuts actually small sized unsalted. Not sure error vendor intended represent product \"Jumbo\".']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### classical way of removing the lambda expressions\n",
    "### verified the output of lambda expression output with the output of following implementation, outputs are same\n",
    "# docs_without_stop_words = []\n",
    "# for i, doc in enumerate(corpus):\n",
    "#     non_stop_words_in_doc = []\n",
    "#     for word in doc.split():\n",
    "#         if word not in stopwords:\n",
    "#             non_stop_words_in_doc.append(word)\n",
    "            \n",
    "    \n",
    "#     docs_without_stop_words.append(' '.join(non_stop_words_in_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming the words (SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus = list(map(lambda doc: ' '.join(list(map(stemmer.stem, doc.split()))), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i have bought sever of the vital can dog food product and have found them all to be of good quality. the product look more like a stew than a process meat and it smell better. my labrador is finicki and she appreci this product better than most.',\n",
       " 'product arriv label as jumbo salt peanuts...th peanut were actual small size unsalted. not sure if this was an error or if the vendor intend to repres the product as \"jumbo\".',\n",
       " 'this is a confect that has been around a few centuries. it is a light, pillowi citrus gelatin with nut - in this case filberts. and it is cut into tini squar and then liber coat with powder sugar. and it is a tini mouth of heaven. not too chewy, and veri flavorful. i high recommend this yummi treat. if you are familiar with the stori of c.s. lewi \"the lion, the witch, and the wardrobe\" - this is the treat that seduc edmund into sell out his brother and sister to the witch.']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_filtered_corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the dataset according to Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_deduplicated_df['Text'] = stemmed_filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = sampled_deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_sorted = working_df.sort_values(by = 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_filtered_corpus_sorted = working_df_sorted['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the reviews and splitting into train, cv and test sets and TRAINING and TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Bag of Words (CountVectorizer)\n",
    "\n",
    "#### Note: The vectorization should be done on the training set and not the entire dataset, if the entire dataset is used for vectorization it is called as Memory leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the vector if a document_term_matrix was already computed earlier, and saved in local dir\n",
    "### if not saved earlier, then fit a CountVectorizer, and then transform on the dataset to obtain the\n",
    "### document_term_matrix, and then save the document_term_matrix\n",
    "\n",
    "if os.path.exists('document_term_matrix_pickle_bow.pkl'):\n",
    "    with open('document_term_matrix_pickle_bow.pkl', 'rb') as document_term_matrix_pickle:\n",
    "        document_term_matrix = pickle.load(document_term_matrix_pickle)\n",
    "else:\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    ### X_train and y_train will be used throughout this prgram, we shall need it always\n",
    "    X_train, X_test, y_train, y_test = train_test_splitter(stemmed_filtered_corpus_sorted, working_df_sorted['Score'], test_size = 0.2)\n",
    "    \n",
    "    document_term_matrix = count_vectorizer.fit_transform(X_train)\n",
    "    with open('document_term_matrix_pickle_bow.pkl', 'wb') as document_term_matrix_pickle:\n",
    "        pickle.dump(document_term_matrix, document_term_matrix_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7579, 15706)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've got the X_train and y_train for now, and also X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow_repr = document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow_repr = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Splitting into train, cv and test (Simple Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will not work because train_test_split() splits data randomly. What we want is a time-based splitting on\n",
    "### the dataset that we have sorted chronologically\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7579, 15706) (9473,)\n"
     ]
    }
   ],
   "source": [
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X     y\n",
      " (7579, 15706) (9473,)\n",
      "(1515, 15706) (3409,)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_splitter(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7579"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the previous X_train and y_train into X_train, X_cv and y_train, y_cv\n",
    "X_train_bow_repr, X_cv_bow_repr, y_train_bow_repr, y_cv_repr = train_test_splitter(X_train_bow_, y_train_bow_repr, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Brute force k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for k = 1 is 87.88167938931298\n",
      "F1-score for k = 3 is 90.24839006439743\n",
      "F1-score for k = 5 is 90.91734786557674\n",
      "F1-score for k = 7 is 90.9090909090909\n",
      "F1-score for k = 9 is 90.990990990991\n",
      "F1-score for k = 11 is 91.08108108108108\n",
      "F1-score for k = 13 is 91.27697841726618\n",
      "F1-score for k = 15 is 91.19496855345912\n",
      "F1-score for k = 17 is 91.24382577458464\n",
      "F1-score for k = 19 is 91.20287253141831\n",
      "F1-score for k = 21 is 91.20287253141831\n",
      "F1-score for k = 23 is 91.20287253141831\n",
      "F1-score for k = 25 is 91.15401885945218\n",
      "F1-score for k = 27 is 91.15401885945218\n",
      "F1-score for k = 29 is 91.15401885945218\n"
     ]
    }
   ],
   "source": [
    "save_name = 'bow_brute_knn'\n",
    "f1scores_for_diff_k = []\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train_bow_repr, y_train_bow_repr, X_cv_repr, y_cv_repr, algorithm = 'brute', save_name = save_name + str(i)))\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1score = max(f1scores_for_diff_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.27697841726618 6\n"
     ]
    }
   ],
   "source": [
    "print(max_f1score, f1scores_for_diff_k.index(max_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### f1-score at index 6 is maximum, from the above list of f1-scores for different k values, we can find that index\\\n",
    "### 5 corresponds to k = 13, ie, hyperparameter k has been tuned to 11\n",
    "\n",
    "## since the model has already been trained and saved with 'k' value suffixed to the file name, we directly load it\n",
    "with open(save_name + str(13) + '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "    \n",
    "# knn = KNeighborsClassifier(n_neighbors = 11, algorithm = 'brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### commenting the following code because knn object loaded in the previous cell is already trained\n",
    "# knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow_repr = count_vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### finally testing after tuning the hyperparameter 'k' on the cross validation set\n",
    "y_pred_test = knn.predict(X_test_bow_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3409, 1515]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-182614ffc516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3409, 1515]"
     ]
    }
   ],
   "source": [
    "f1_score(y_test, y_pred_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5. Kd-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'bow_kd_knn'\n",
    "f1scores_for_diff_k = []\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train_bow_repr, y_train_bow_repr, X_cv_bow_repr, y_cv_bow_repr), algorithm = 'kd_tree', save_name = (save_name + str(i)))\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1score = max(f1scores_for_diff_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_f1score, f1scores_for_diff_k.index(max_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### k = 11 at index 5, ie, k value is tuned to 11\n",
    "\n",
    "with open(save_name + str(11) + '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickel.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 11, algorithm = 'kd_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### finally testing after tuning the hyperparameter 'k' on the cross validation set\n",
    "y_pred_test = knn.predict(X_test_bow_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the vector if a document_term_matrix was already computed earlier, and saved in local dir\n",
    "### if not saved earlier, then fit a TfIdfVectorizer, and then transform on the dataset to obtain the\n",
    "### document_term_matrix, and then save the document_term_matrix\n",
    "\n",
    "if os.path.exists('document_term_matrix_pickle_tfidf.pkl'):\n",
    "    with open('document_term_matrix_pickle_tfidf.pkl', 'rb') as document_term_matrix_pickle:\n",
    "        document_term_matrix = pickle.load(document_term_matrix_pickle)\n",
    "else:\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,1))\n",
    "#     X_train, X_test, y_train, y_test = train_test_splitter(stemmed_filtered_corpus_sorted, stemm, test_size = 0.2, return_only_training_split = True)\n",
    "    tfidf_vectorizer = tfidf_vectorizer.fit(X_train)\n",
    "    document_term_matrix = tfidf_vectorizer.transform(X_train)\n",
    "    with open('document_term_matrix_pickle_tfidf.pkl', 'wb') as document_term_matrix_pickle:\n",
    "        pickle.dump(document_term_matrix, document_term_matrix_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = tfidf_vectorizer.fit(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_term_matrix = tfidf_vectorizer.transform(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_repr = document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tfidf_repr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_repr, X_cv_tfidf_repr, y_train_tfidf_repr, y_cv_tfidf_repr = train_test_splitter(X_train_tfidf_repr, y_train_tfidf_repr, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Brute force k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'tfidf_brute_knn'\n",
    "f1scores_for_diff_k = []\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train_tfidf_repr, y_train_tfidf_repr, X_cv_tfidf_repr, y_cv_tfidf_repr, algorithm = 'brute', save_name = save_name + str(i))\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(f1scores_for_diff_k), f1scores_for_diff_k.index(max(f1scores_for_diff_k)), sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index = 6 has the maximum f1_score. This index corresponds to k = 13 (from the above printed list of f1scores)\n",
    "\n",
    "with open(save_name + str(13) + '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 13, algorithm = 'brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the final 13-NN\n",
    "\n",
    "# knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_repr = tfidf_vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing on test data\n",
    "\n",
    "y_pred = knn.predict(X_test_tfidf_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### f1_score score\n",
    "\n",
    "f1_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. kd_tree k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'tfidf_kd_knn'\n",
    "f1scores_for_diff_k = []\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train_tfidf_repr, y_train_repr, X_cv_repr, y_cv_repr, algorithm = 'kd_tree', save_name = save_name + str(i)\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(f1scores_for_diff_k), f1scores_for_diff_k.index(max(f1scores_for_diff_k)), sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index = 10 has the maximum f1_score. This index corresponds to k = 21 (from the above printed list of f1scores)\n",
    "\n",
    "with open(save_name + str(21), '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 21, algorithm = 'kd_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the final 13-NN\n",
    "\n",
    "# knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing on test data\n",
    "\n",
    "y_pred = knn.predict(X_test_tffidf_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### f1_score score\n",
    "\n",
    "f1_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Average W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tokenizing each document in the corpus\n",
    "#### gensim w2v requires each document to be tokenized into words. The corpus will be a list of lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemmed_filtered_corpus_sorted is a pandas Series object. It should be converted into a list first.\n",
    "## after that each sentence in the resulted list should be tokenized into words stored in a list.\n",
    "## all these lists should be stored into another list so as to give a list of lists as required by gensim w2v\n",
    "# X_train, y_train = train_test_splitter(stemmed_filtered_corpus_sorted, test_size = 0.2, return_only_training_split = True)\n",
    "# stemmed_filtered_corpus_sorted_list = list(stemmed_filtered_corpus_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_test_splitter(stemmed_filtered_corpus_sorted, None, test_size = 0.2, return_only_training_split = True)\n",
    "\n",
    "stemmed_filtered_sorted_list_of_tokenized_sentences = []\n",
    "\n",
    "for sentence in X_train:\n",
    "    tokenized_sentence = sentence.split()\n",
    "    stemmed_filtered_sorted_list_of_tokenized_sentences.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stemmed_filtered_sorted_list_of_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the vector if a w2v model was already trained earlier, and saved in local dir\n",
    "### if not saved earlier, then fit a Word2Vec model, on the dataset and then save it\n",
    "\n",
    "if os.path.exists('word2vec_trained_model.pkl'):\n",
    "    with open('word2vec_trained_model.pkl', 'rb') as word2vec_trained_model:\n",
    "        word2vec_trained_model = pickle.load(word2vec_trained_model)\n",
    "else:\n",
    "    w2v = gensim.models.Word2Vec(stemmed_filtered_sorted_list_of_tokenized_sentences, min_count = 1, size = 50, workers = 4)\n",
    "    with open('word2vec_trained_model.pkl', 'wb') as word2vec_trained_model:\n",
    "        pickle.dump(w2v, word2vec_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = gensim.models.Word2Vec(stemmed_filtered_sorted_list_of_tokenized_sentences, min_count = 1, size = 50, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['happen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(w2v.wv['happen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['happen'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### saving the w2v model for later use\n",
    "\n",
    "# import os\n",
    "\n",
    "# if(not os.path.exists('w2v_practice.model')):\n",
    "#     w2v.save('w2v_practice.model')\n",
    "    \n",
    "# else:\n",
    "#     w2v = gensim.models.Word2Vec.load('w2v_practice.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the vector if a avg_w2v representation of each sentence was already computed earlier and saved in local dir\n",
    "### if not saved earlier, then compute an avg_w2v representation for all the sentences and then save the list\n",
    "\n",
    "if os.path.exists('avg_word2vec.pkl'):\n",
    "    with open('avg_word2vec.pkl', 'rb') as avg_w2v_pkl:\n",
    "        avg_w2v = pickle.load(avg_w2v_pkl)\n",
    "else:\n",
    "    avg_w2v = []\n",
    "    for tokenized_sentence in stemmed_filtered_sorted_list_of_tokenized_sentences:\n",
    "        sum_of_vectors_for_each_word = 0\n",
    "        for word in tokenized_sentence:\n",
    "            sum_of_vectors_for_each_word += w2v.wv[word]\n",
    "        avg_w2v.append(sum_of_vectors_for_each_word / len(tokenized_sentence))\n",
    "    with open('avg_word2vec.pkl', 'wb') as avg_w2v_pkl:\n",
    "        pickle.dump(avg_w2v, avg_w2v_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### computing avg w2v representation for the reviews dataset\n",
    "\n",
    "# avg_w2v = []\n",
    "# for tokenized_sentence in stemmed_filtered_sorted_list_of_tokenized_sentences:\n",
    "#     sum_of_vectors_for_each_word = 0\n",
    "#     for word in tokenized_sentence:\n",
    "#         sum_of_vectors_for_each_word += w2v.wv[word]\n",
    "#     avg_w2v.append(sum_of_vectors_for_each_word / len(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(avg_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w2v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = avg_w2v\n",
    "## y is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the train_test_splitter() assumes X to be a numpy array\n",
    "X = np.array(avg_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_first, X_test, y_train_first, y_test = train_test_splitter(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_splitter(X_train_first, y_train_first, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Brute force k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores_for_diff_k = []\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train, y_train, X_cv, y_cv, algorithm = 'brute', save_name = 'avgw2v_brute_knn')\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1score = max(f1scores_for_diff_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_f1score, f1scores_for_diff_k.index(max_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### max_f1_score is indexed at 5 which corresponds to k = 11; hypertuned on CV set. Training knn for k = 5\n",
    "\n",
    "with open(save_name + str(11) + '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 5, algorithm = 'brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. kd_tree k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores_for_diff_k = []\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train, y_train, X_cv, y_cv, algorithm = 'kd_tree', save_name = 'avgw2v_kd_knn')\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1score = max(f1scores_for_diff_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_f1score, f1scores_for_diff_k.index(max_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### max_f1_score is indexed at 5 which corresponds to k = 11; hypertuned on CV set. Training knn for k = 5\n",
    "\n",
    "with open(save_name + str(11) + '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 11, algorithm = 'kd_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. TfIdf weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the vector if a tfidf_weighted_w2v representation of each sentence was already computed earlier and saved\n",
    "### if not saved earlier, then compute an tfidf_weighted_w2v representation for all the sentences and then save\n",
    "### the list of representation of each sentence\n",
    "\n",
    "is os.path.exists('tfidf_weighted_word2vec.pkl'):\n",
    "    with open('tfidf_weighted_word2vec.pkl', 'rb') as tfidf_weighted_w2v_pkl:\n",
    "        tfidf_weighted_w2v = pickle.load(tfidf_weighted_w2v_pkl)\n",
    "else:\n",
    "    tfidf_weighted_w2v = []\n",
    "    for sentence in stemmed_filtered_sorted_list_of_tokenized_sentences:\n",
    "        tfidf_weighted_sum_of_vectors_for_each_word = 0\n",
    "        for word in sentence:\n",
    "            if word not in tfidf_vectorizer.vocabulary_ or word not in w2v.wv:\n",
    "                continue\n",
    "            tfidf_weighted_sum_of_vectors_for_each_word += tfidf_vectorizer.vocabulary_[word] * w2v.wv[word]\n",
    "        tfidf_weighted_w2v.append(tfidf_weighted_sum_of_vectors_for_each_word)\n",
    "    with open('tfidf_weighted_word2vec.pkl', 'wb') as tfidf_weighted_w2v_pkl:\n",
    "        pickle.dump(tfidf_weighted_w2v, tfidf_weighted_w2v_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_weighted_w2v = []\n",
    "# for sentence in stemmed_filtered_sorted_list_of_tokenized_sentences:\n",
    "#     tfidf_weighted_sum_of_vectors_for_each_word = 0\n",
    "#     for word in sentence:\n",
    "#         if word not in tfidf_vectorizer.vocabulary_ or word not in w2v.wv:\n",
    "#             continue\n",
    "#         tfidf_weighted_sum_of_vectors_for_each_word += tfidf_vectorizer.vocabulary_[word] * w2v.wv[word]\n",
    "#     tfidf_weighted_w2v.append(tfidf_weighted_sum_of_vectors_for_each_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tfidf_weighted_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_first, X_test, y_train_first, y_test = train_test_splitter(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_splitter(X_train_first, y_train_first, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Brute force k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores_for_diff_k = []\n",
    "\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train, y_train, X_cv, y_cv, algorithm = 'brute', save_name = 'tfidf_brute_knn')\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1score = max(f1scores_for_diff_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_f1score, f1scores_for_diff_k.index(max_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index = 10 corresponds to k = 21, ie, hyperparamter k is tuned to 21\n",
    "\n",
    "with open(save_name + str(21) + '.pkl', 'rb') ass knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 21, algorithm = 'brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. kd_tree k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores_for_diff_k = []\n",
    "\n",
    "for i in range(1, 30, 2):\n",
    "    f1score = knn_trainer_and_cross_validator(i, X_train, y_train, X_cv, y_cv, algorithm = 'kd_tree', save_name = 'tfidf_kd_knn')\n",
    "    f1scores_for_diff_k.append(f1score)\n",
    "    print('F1-score for k = ' + str(i) + ' is ' + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1score = max(f1scores_for_diff_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_f1score, f1scores_for_diff_k.index(max_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index = 10 corresponds to k = 21, ie, hyperparamter k is tuned to 21\n",
    "\n",
    "with open(save_name + str(21) + '.pkl', 'rb') as knn_pkl:\n",
    "    knn = pickle.load(knn_pkl)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 21, algorithm = 'kd_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_table = PrettyTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_table.field_names = ['Vectorizer', 'Model', 'Hyperparameter (k) value', 'F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_table.add_row(['BoW', 'Brute k-NN', '11', '90.84'])\n",
    "pretty_table.add_row(['BoW', 'kd_tree k-NN', '11', '90.823'])\n",
    "pretty_table.add_row(['Tf-Idf', 'Brute k-NN', '13', '91.063'])\n",
    "pretty_table.add_row(['Tf-Idf', 'kd_tree k-NN', '21', '90.885'])\n",
    "pretty_table.add_row(['Avg_W2V', 'Brute k-NN', '11', '89.13'])\n",
    "pretty_table.add_row(['Avg_W2V', 'kd_tree k-NN', '11', '90.187'])\n",
    "pretty_table.add_row(['Tf-Idf_W2V', 'Brute k-NN', '21', '90.885'])\n",
    "pretty_table.add_row(['Tf-Idf_W2V', 'kd_tree k-NN', '21', '90.885'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretty_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
