{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the sql dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "connection = sqlite3.connect('database.sqlite')\n",
    "\n",
    "# polarisable_dataset = dataset that contains Score = {1,2,4,5} assuming Score = 3 implies neutral comments and\n",
    "# Score < 3 implies negative comment and Score > 3 implies positive comment\n",
    "polarisable_dataset = pd.read_sql_query('select * from REVIEWS WHERE Score != 3', connection)\n",
    "polarisable_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace values in Score column in polarisable dataset with 'positive' and 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = polarisable_dataset['Score']\n",
    "\n",
    "polarised_scores = scores.map(lambda x: 0 if x<3 else 1)\n",
    "\n",
    "# polarised_scores.head()\n",
    "\n",
    "polarisable_dataset['Score'] = polarised_scores\n",
    "polarised_dataset = polarisable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarised_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Deduplication\n",
    "If a user id has multiple entries for the same timestamp, then it should be removed because it is likely that multiple entries at the same timestamp were for the same product of different variety which has a different product id than other variants\n",
    "\n",
    "##### Observation 1- There are 197082 duplicate entries (using rule-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197082\n"
     ]
    }
   ],
   "source": [
    "print(polarised_dataset.duplicated(['UserId', 'Time']).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328732, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_dataset = polarised_dataset.drop_duplicates(subset = {'UserId', 'Time'}, keep = 'first', inplace = False)\n",
    "deduplicated_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Text preprocessing- Removing html and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have bought several of the Vitality canned d...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_dataset['Text'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset cleaners\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_html(sentence):\n",
    "    html_tag_re_obj = re.compile('<.*>?')\n",
    "    return re.sub(html_tag_re_obj, ' ', sentence)\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z]', r' ', sentence)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "\n",
    "corpus = deduplicated_dataset['Text']\n",
    "\n",
    "cleaned_corpus = []\n",
    "for doc in corpus.values:\n",
    "    cleaned_doc = remove_html(doc)\n",
    "    cleaned_doc = remove_punctuations(cleaned_doc)\n",
    "    cleaned_corpus.append(cleaned_doc)\n",
    "\n",
    "deduplicated_dataset['Text'] = cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Text preprocessing- Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'yours', 'should', 'but', 'by', 'be', 'down', 'hasn', \"she's\", 'has', 'with', 'no', 'can', 'now', 'any', \"hadn't\", 'wouldn', 'theirs', 'at', 'itself', 'most', 't', 'did', \"weren't\", \"doesn't\", 'we', 'some', \"isn't\", 'yourselves', 'mustn', 'our', 'very', 'her', 'for', 'doesn', 'on', 'then', 'have', 've', 'couldn', 'ma', 'about', 'him', 'so', 'their', 'off', 'she', 'those', 'having', 'if', 'its', 'when', 'do', \"couldn't\", 'was', 'won', 'before', 'out', 'being', 'y', 'ourselves', 'an', 'the', 'mightn', 'does', 'such', 'other', 'your', 're', 'or', 'shan', 'why', 'his', 'had', 'than', 'ain', 'through', 'am', \"it's\", 'are', 'same', 'whom', 'haven', 'from', 'over', 'until', \"wasn't\", 'herself', 'my', 'what', 'don', 'themselves', 'in', 'aren', 'between', 'you', 'it', 'where', \"mustn't\", 'again', \"won't\", 'that', 'each', 'after', 'doing', \"haven't\", 'not', 'll', \"you've\", 'is', \"shouldn't\", 'more', 'me', \"should've\", \"shan't\", 'o', 'further', 'shouldn', 'he', 'hadn', \"hasn't\", 'were', 'myself', 'up', 'them', 'there', 'under', \"don't\", 'to', 'as', 'wasn', 'own', 'too', 'above', 'which', 'while', \"that'll\", 'just', 's', \"you'd\", 'once', 'd', \"aren't\", \"mightn't\", 'and', 'few', 'ours', 'i', 'against', \"you're\", 'both', \"needn't\", 'this', 'been', 'into', \"didn't\", 'how', 'they', \"wouldn't\", 'only', 'himself', 'of', 'here', 'all', 'will', 'didn', 'weren', 'these', 'isn', 'during', 'who', 'needn', 'below', 'm', 'a', 'nor', 'because', \"you'll\", 'hers'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104301\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Since the negative food reviews are likely to contain words like \"don't\", \"didn't\", etc that impart important\n",
    "## meaning to the review, we check if such words exist in the corpus that we have. If these words are in the corpus,\n",
    "## then they should not be in the list of stop words that we use for removing the stopwords from our corpus\n",
    "\n",
    "count = 0\n",
    "for doc in deduplicated_dataset['Text']:\n",
    "    if \"not\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in deduplicated_dataset['Text']:\n",
    "    if \"don't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for doc in deduplicated_dataset['Text']:\n",
    "    if \"didn't\" in doc:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'not' is present in 104301 docs in the corpus, so we modify the list of stopwords to not contain this word\n",
    "\n",
    "stopwords_set = set(stopwords)\n",
    "\n",
    "stopwords_set.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'yours', 'should', 'but', 'by', 'be', 'down', 'hasn', \"she's\", 'has', 'with', 'no', 'can', 'now', 'any', \"hadn't\", 'wouldn', 'theirs', 'at', 'itself', 'most', 't', 'did', \"weren't\", \"doesn't\", 'we', 'some', \"isn't\", 'yourselves', 'mustn', 'our', 'very', 'her', 'for', 'doesn', 'on', 'then', 'have', 've', 'couldn', 'ma', 'about', 'him', 'so', 'their', 'off', 'she', 'those', 'having', 'if', 'its', 'when', 'do', \"couldn't\", 'was', 'won', 'before', 'out', 'being', 'y', 'ourselves', 'an', 'the', 'mightn', 'does', 'such', 'other', 'your', 're', 'or', 'shan', 'why', 'his', 'had', 'than', 'ain', 'through', 'am', \"it's\", 'are', 'same', 'whom', 'haven', 'from', 'over', 'until', \"wasn't\", 'herself', 'my', 'what', 'don', 'themselves', 'in', 'aren', 'between', 'you', 'it', 'where', \"mustn't\", 'again', \"won't\", 'that', 'each', 'after', 'doing', \"haven't\", 'll', \"you've\", 'is', \"shouldn't\", 'more', 'me', \"should've\", \"shan't\", 'o', 'further', 'shouldn', 'he', 'hadn', \"hasn't\", 'were', 'myself', 'up', 'them', 'there', 'under', \"don't\", 'to', 'as', 'wasn', 'own', 'too', 'above', 'which', 'while', \"that'll\", 'just', 's', \"you'd\", 'once', 'd', \"aren't\", \"mightn't\", 'and', 'few', 'ours', 'i', 'against', \"you're\", 'both', \"needn't\", 'this', 'been', 'into', \"didn't\", 'how', 'they', \"wouldn't\", 'only', 'himself', 'of', 'here', 'all', 'will', 'didn', 'weren', 'these', 'isn', 'during', 'who', 'needn', 'below', 'm', 'a', 'nor', 'because', \"you'll\", 'hers'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lower casing all docs in corpus (deduplicated_dataset['Text'])\n",
    "\n",
    "lower_cased_docs = [doc.lower() for doc in deduplicated_dataset['Text']]\n",
    "deduplicated_dataset['Text'] = lower_cased_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "corpus = deduplicated_dataset['Text'] # corpus contains cleaned docs\n",
    "print(type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_without_stop_words = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    non_stop_words_in_doc = []\n",
    "    for word in doc.split():\n",
    "        if word not in stopwords_set:\n",
    "            non_stop_words_in_doc.append(word)\n",
    "            \n",
    "    \n",
    "    docs_without_stop_words.append(' '.join(non_stop_words_in_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted not sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven not chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal', 'got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much red black licorice flavored pieces not particular favorites kids husband lasted two weeks would recommend brand taffy delightful treat', 'saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party everyone loved', 'taffy good soft chewy flavors amazing would definitely recommend buying satisfying', 'right mostly sprouting cats eat grass love rotate around wheatgrass rye', 'healthy dog food good digestion also good small puppies dog eats required amount every feeding']\n"
     ]
    }
   ],
   "source": [
    "print(docs_without_stop_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Text preprocessing- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'found'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_corpus = [] # docs with stemmed words\n",
    "for doc in docs_without_stop_words:\n",
    "    stemmed_words = []\n",
    "    for word in doc.split():\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    stemmed_doc = ' '.join(stemmed_words)\n",
    "    stemmed_corpus.append(stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought sever vital can dog food product found good qualiti product look like stew process meat smell better labrador finicki appreci product better', 'product arriv label jumbo salt peanut peanut actual small size unsalt not sure error vendor intend repres product jumbo', 'confect around centuri light pillowi citrus gelatin nut case filbert cut tini squar liber coat powder sugar tini mouth heaven not chewi flavor high recommend yummi treat familiar stori c lewi lion witch wardrob treat seduc edmund sell brother sister witch', 'look secret ingredi robitussin believ found got addit root beer extract order good made cherri soda flavor medicin', 'great taffi great price wide assort yummi taffi deliveri quick taffi lover deal', 'got wild hair taffi order five pound bag taffi enjoy mani flavor watermelon root beer melon peppermint grape etc complaint bit much red black licoric flavor piec not particular favorit kid husband last two week would recommend brand taffi delight treat', 'saltwat taffi great flavor soft chewi candi individu wrap well none candi stuck togeth happen expens version fraling would high recommend candi serv beach theme parti everyon love', 'taffi good soft chewi flavor amaz would definit recommend buy satisfi', 'right most sprout cat eat grass love rotat around wheatgrass rye', 'healthi dog food good digest also good small puppi dog eat requir amount everi feed']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
